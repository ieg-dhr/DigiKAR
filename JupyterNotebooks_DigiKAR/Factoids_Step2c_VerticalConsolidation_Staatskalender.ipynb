{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGx3Qz75zaQ2ZoqR/7Elql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/DigiKAR/blob/main/JupyterNotebooks_DigiKAR/Factoids_Step2c_VerticalConsolidation_Staatskalender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a script for consolidating factoid lists in AP3.\n",
        "\n",
        "The package mainly uses the Pandas package in Python to read and manipulate EXCEL data as DataFrames. DataFrames are 2-dimensional data representations in rows and columns. They can be written to different file formats such as CSV, EXCEL, JSON or RDF.\n",
        "\n",
        "First of all, we need to connect this Colab notebook with your Google Drive and define the directory for input and output data.\n"
      ],
      "metadata": {
        "id": "S3ydZRhYATDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "directory=\"/content/drive/My Drive/Colab_DigiKAR/\""
      ],
      "metadata": {
        "id": "39qqRJOgZmPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second step, we have to install additional Packages needed for working with CSV, EXCEL and DataFrames."
      ],
      "metadata": {
        "id": "dCAdylkZL9f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## install packages that are not part of Python's standard distribution\n",
        "\n",
        "!pip install xlsxwriter\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3d6OjjlTZ2ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 1**, we can import the packages to the script and load our data. Before merging the input files, names will be normalised as some have access spaces, capitalised surnames, or inverted first and last names.\n",
        "\n",
        "The combined data will be written to a new dataframe and displayed."
      ],
      "metadata": {
        "id": "agRRVOwiOWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlsxwriter\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# path to input files\n",
        "\n",
        "factoid_paths=[\"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/FactoidList_1756er_Staatskalender_Meta_final_TEST-MB_FS0.xlsx\",\n",
        "               \"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/FactoidList_1756er_Staatskalender_Meta_final_TEST-MB_FS1.xlsx\",\n",
        "               \"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/FactoidList_1756er_Staatskalender_Meta_final_TEST-MB_FS2.xlsx\",\n",
        "               \"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/FactoidList_1756er_Staatskalender_Meta_final_TEST-MB_FS3.xlsx\",\n",
        "               \"https://github.com/ieg-dhr/DigiKAR/raw/main/Sample%20Data/FactoidList_1756er_Staatskalender_Meta_final_TEST-MB_FS4.xlsx\"\n",
        "               ]\n",
        "\n",
        "# define dataframe for final output\n",
        "\n",
        "f_to_add=[]\n",
        "\n",
        "# structure of input files\n",
        "\n",
        "# obligatory columns in valid factoid list\n",
        "\n",
        "# read all data frames from path\n",
        "\n",
        "frame_list=[]\n",
        "for file in factoid_paths:\n",
        "    df = pd.read_excel(file, index_col=None, dtype=str) # axis=1, sort=False sheet_name='FactoidList'\n",
        "    df = df.fillna(\"n/a\") # replace empty fields for string\n",
        "    df_length=len(df)\n",
        "    frame_list.append(df)\n",
        "\n",
        "f = pd.concat(frame_list, axis=0, ignore_index=True, sort=False)\n",
        "\n",
        "print(\"There are \", len(f), \"items in your DataFrame!\")\n",
        "\n",
        "# delete all duplicate rows with exact matches\n",
        "\n",
        "f_unique=f.drop_duplicates()\n",
        "print(\"Your DataFrame has now \", len(f_unique), \"items with at least one unique cell.\" )\n",
        "\n",
        "# add columns missing according to factoid model\n",
        "\n",
        "column_names = [\"factoid_ID\",\n",
        "                \"pers_ID\",\n",
        "                \"pers_name\",\n",
        "                \"alternative_names\",\n",
        "                \"event_type\",\n",
        "                \"event_after-date\",\n",
        "                \"event_before-date\",\n",
        "                \"event_start\",\n",
        "                \"event_end\",\n",
        "                \"event_date\",\n",
        "                \"pers_title\",\n",
        "                \"pers_function\",\n",
        "                \"place_name\",\n",
        "                \"inst_name\",\n",
        "                \"rel_pers\",\n",
        "                \"source_quotations\",\n",
        "                \"additional_info\",\n",
        "                \"comment\",\n",
        "                \"info_dump\",\n",
        "                \"source_combined\",\n",
        "                \"event_value\", # add more potential categorisations if needed\n",
        "                \"source\",\n",
        "                \"source_site\"]\n",
        "\n",
        "df2 = f_unique.reindex(columns=column_names)\n",
        "df2.fillna('n/a', inplace=True)\n",
        "\n",
        "# populate some of the empty columns with data\n",
        "\n",
        "df2.loc[:, \"event_end\"] = df2[\"event_start\"]\n",
        "df2.loc[:, \"event_type\"] = [\"Funktionsausübung\"] * 31414\n",
        "df2['source_combined'] = df2['source'].astype(str) + ': ' + df2['source_site'].astype(str)\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "# rename dataframe for next step\n",
        "\n",
        "display(df2)\n"
      ],
      "metadata": {
        "id": "zYkO-hJ7rxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge dataframe with dfs containing person IDs and geocoding\n",
        "\n",
        "## Read person IDs from Github\n",
        "#infile1=directory+\"Mainz2_Geonames.xlsx\"\n",
        "#person_df = pd.read_excel(infile1)\n",
        "\n",
        "## Read geocoding from Github\n",
        "#infile2=directory+\"Addresses_Geocoded_withGoogle.xlsx\"\n",
        "#geo_df = pd.read_excel(infile2)\n",
        "\n",
        "## Merge input dataframe horizontally\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# define list of DataFrames\n",
        "#dfs = [df2, person_df, geo_df] # dataframe list can be extended if necessary\n",
        "\n",
        "# merge all DataFrames into one\n",
        "#final_df = reduce(lambda  left,right: pd.merge(dfs) # left,right,on=['column_name'] # how='outer'\n",
        "\n",
        "#display(final_df)\n",
        "\n",
        "## Write new table to excel file\n",
        "\n",
        "#outfile=directory+\"AP3_final-df.xlsx\"\n",
        "#final_df.to_excel(outfile)"
      ],
      "metadata": {
        "id": "af_2sGejn098"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 2**, we reconstruct end dates for successive start dates. The data are automatically aggregated using Python's `groupby` function. If the results are too narrow or too broad, please change the aggregation rules below!\n"
      ],
      "metadata": {
        "id": "g-rqManxlTZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the dataframe and aggregate the start and end dates\n",
        "# code updated after problem with merged columns\n",
        "# see discussion on Stackoverflow: https://stackoverflow.com/questions/76558443/column-remains-empty-when-using-map-with-dictionary-in-pandas-dataframe/76558586#76558586\n",
        "\n",
        "grouped_df = df2.groupby(['pers_name', 'event_type', \"pers_function\", \"pers_title\", \"inst_name\", \"place_name\"], as_index=False).agg(\n",
        "                                                         {'event_start': 'min',\n",
        "                                                          \"event_after-date\":'min',\n",
        "                                                          \"event_before-date\":'max',\n",
        "                                                          \"event_end\":'max',\n",
        "                                                          \"factoid_ID\":list,\n",
        "                                                          \"alternative_names\":list,\n",
        "                                                          \"pers_ID\":list,\n",
        "                                                          \"rel_pers\":list,\n",
        "                                                          \"source_quotations\":list,\n",
        "                                                          \"additional_info\":list,\n",
        "                                                          \"comment\":list,\n",
        "                                                          \"info_dump\":list,\n",
        "                                                          \"source_combined\":list,\n",
        "                                                          \"event_value\":list\n",
        "                                                          })\n",
        "\n",
        "display(grouped_df)"
      ],
      "metadata": {
        "id": "dX6-mOCKlYHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 3**, we can flatten the information and only preserve unique information per cell."
      ],
      "metadata": {
        "id": "9Gwx0fvvUXr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten data in dataframe cells\n",
        "\n",
        "def flatten_list(cell):\n",
        "    if isinstance(cell, list):\n",
        "        unique_values = set(cell)\n",
        "        return ', '.join(str(value) for value in unique_values)\n",
        "    else:\n",
        "        return str(cell)\n",
        "\n",
        "# flatten all cells containing lists\n",
        "df3 = grouped_df.applymap(flatten_list)\n",
        "\n",
        "# show the flattened DataFrame\n",
        "display(df3)"
      ],
      "metadata": {
        "id": "qTMJTpaFxQ19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **step 4**, we enrich the data, e.g. by adding event values from an external Python dictionary stored in Github."
      ],
      "metadata": {
        "id": "6_e5794SUmpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load external dictionary with EVENT VALUES\n",
        "# following method 2 on https://www.geeksforgeeks.org/how-to-read-dictionary-from-file-in-python/\n",
        "\n",
        "# importing the module\n",
        "import requests\n",
        "import ast\n",
        "\n",
        "master = \"https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/Data%20Categorisation/Event_value_dict.txt\" # add Sven's new mapping\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "print(req)\n",
        "\n",
        "# reconstructing the data as a dictionary\n",
        "event_value_dict = ast.literal_eval(req)\n",
        "print(type(event_value_dict))\n",
        "\n",
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    test = event_value_dict[\"Aufschwörung\"] # random test if valid dict\n",
        "    print(\"Value for chosen key: \", test)\n",
        "except:\n",
        "    print(\"Invalid dict structure!\")\n",
        "\n",
        "df3['event_value'] = df3['event_type'].map(event_value_dict) # optional: na_action='ignore'\n",
        "\n",
        "display(df3)"
      ],
      "metadata": {
        "id": "iL7uHngsU59m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load external dictionary with EVENT CATEGORIES (e.g. I: agent-oriented)\n",
        "# following method 2 on https://www.geeksforgeeks.org/how-to-read-dictionary-from-file-in-python/\n",
        "\n",
        "# importing the module\n",
        "import requests\n",
        "import ast\n",
        "\n",
        "master = \"https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/Data%20Categorisation/####.txt\" # add file name\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "print(req)\n",
        "\n",
        "# reconstructing the data as a dictionary\n",
        "event_category_dict = ast.literal_eval(req)\n",
        "print(type(event_category_dict))\n",
        "\n",
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    test = event_category_dict[\"Geburt\"] # random test if valid dict\n",
        "    print(\"Value for chosen key: \", test)\n",
        "except:\n",
        "    print(\"Invalid dict structure!\")\n",
        "\n",
        "df3['event_category'] = df3['event_type'].map(event_category_dict) # optional: na_action='ignore'\n",
        "\n",
        "display(df3)"
      ],
      "metadata": {
        "id": "OBO2rWMUpgm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load external dictionary with FUNCTION CATEGORIES (e.g. teaching versus administration)\n",
        "# following method 2 on https://www.geeksforgeeks.org/how-to-read-dictionary-from-file-in-python/\n",
        "\n",
        "# importing the module\n",
        "import requests\n",
        "import ast\n",
        "\n",
        "master = \"https://raw.githubusercontent.com/ieg-dhr/DigiKAR/main/Data%20Categorisation/####.txt\" # add file name\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "print(req)\n",
        "\n",
        "# reconstructing the data as a dictionary\n",
        "function_category_dict = ast.literal_eval(req)\n",
        "print(type(function_category_dict))\n",
        "\n",
        "# add event values from dict to data frame\n",
        "\n",
        "try:\n",
        "    test = function_category_dict[\"Professor\"] # random test if valid dict\n",
        "    print(\"Value for chosen key: \", test)\n",
        "except:\n",
        "    print(\"Invalid dict structure!\")\n",
        "\n",
        "df3['function_category'] = df3['pers_function'].map(function_category_dict) # optional: na_action='ignore'\n",
        "\n",
        "display(df3)"
      ],
      "metadata": {
        "id": "e3tZkxRKqPIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save enriched df to DRIVE\n",
        "\n",
        "workbook=directory+'FACTOIDS_consolidated/Factoid_Staatskalender_ALL_consolidation_with-event-values.xlsx'\n",
        "print(workbook)\n",
        "writer = pd.ExcelWriter(workbook, engine='xlsxwriter') # create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "df3.to_excel(writer, sheet_name='FactCons1') # Convert the dataframe to an XlsxWriter Excel object.\n",
        "writer.save() # Close the Pandas Excel writer and output the Excel file.\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "3mCV6d9FTRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output files and repeat process if necessary.\n",
        "\n",
        "Script by Monika Barget, Maastricht/Mainz\n",
        "\n",
        "June 2023\n"
      ],
      "metadata": {
        "id": "GBSEVpnKXS_u"
      }
    }
  ]
}